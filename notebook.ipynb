{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "notebook.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWIYhmc5gszG"
      },
      "source": [
        "import os,sys,time,math,textwrap\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import dataset, transformer\n",
        "\n",
        "root = 'data'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgvS5ipLgszP"
      },
      "source": [
        "lr = .00035\n",
        "context = 150\n",
        "batch_size = 32\n",
        "log_interval = 50\n",
        "\n",
        "heads = 10\n",
        "depth = 16\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCX8Gfr1gszQ"
      },
      "source": [
        "train_data = dataset.WikiText2(root, context, dataset.DatasetSplit.train)\n",
        "valid_data = dataset.WikiText2(root, context, dataset.DatasetSplit.valid)\n",
        "test_data = dataset.WikiText2(root, context, dataset.DatasetSplit.test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ety3dhgOgszQ"
      },
      "source": [
        "def evaluate(data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = 0.\n",
        "        loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=False)\n",
        "        for i, (x,y) in enumerate(loader):\n",
        "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
        "            yhat = model(x).view(-1, train_data.word_count())\n",
        "            loss += criterion(yhat, y.contiguous().view(-1))\n",
        "\n",
        "    print()\n",
        "    model.train()\n",
        "    return loss / len(loader)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajYUJr72gszR",
        "outputId": "2d061f41-1a5f-40b5-b09e-d549c2c16377"
      },
      "source": [
        "model = transformer.Transformer(context, train_data.word_count(), 400, 40, 900, heads, depth, tied_weights=True).to(device)\n",
        "count = sum([np.prod(parm.shape) for parm in model.parameters() if parm.requires_grad])\n",
        "print('Initialized graph with {} parameters'.format(count))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized graph with 35198479 parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_6012DegszT",
        "outputId": "c0edd429-88e4-43e4-b5f7-9a408f756c79"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "curr_lr = .0001\n",
        "clip = .25\n",
        "best_val_loss = None\n",
        "epochs = 10\n",
        "save = 'model.pt'\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
        "print('Initiating training, {} iterations/epoch.'.format(len(train_loader)))\n",
        "\n",
        "try:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)\n",
        "    for epoch in range(epochs):\n",
        "        t0 = time.time()\n",
        "        val_loss = evaluate(valid_data)\n",
        "        print('-' * 100)\n",
        "        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} | '\n",
        "                'validation perplexity {:8.2f}'.format(epoch, (time.time() - t0),\n",
        "                                                       val_loss, math.exp(val_loss)))\n",
        "        print('-' * 100)\n",
        "        print('epoch\\t\\tms/batch\\tlr\\tloss\\tperplexity')\n",
        "\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0.\n",
        "        t0 = time.time()\n",
        "        if epoch == 1: optimizer.param_groups[0]['lr'] = curr_lr = lr # finished warmup\n",
        "        for i, (x,y) in enumerate(train_loader):\n",
        "            if i % log_interval == 0 and i > 0:\n",
        "                cur_loss = total_loss / log_interval\n",
        "                elapsed = time.time() - t0\n",
        "                print('{:3d} ({:2.1f}%)\\t{:5.2f}\\t\\t{:1.3}\\t{:5.2f}\\t{:8.2f}'.format(\n",
        "                    epoch, 100*i/float(len(train_loader)),\n",
        "                    elapsed * 1000 / log_interval, curr_lr, cur_loss, math.exp(cur_loss)))\n",
        "                total_loss = 0\n",
        "                t0 = time.time()\n",
        "\n",
        "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
        "            model.zero_grad()\n",
        "            yhat = model(x).view(-1, train_data.word_count())\n",
        "            loss = criterion(yhat, y.contiguous().view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('Graceful Exit')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating training, 436 iterations/epoch.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   0 | time: 15.42s | validation loss 10.41 | validation perplexity 33279.18\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  0 (11.5%)\t997.33\t\t0.0001\t 9.66\t15746.70\n",
            "  0 (22.9%)\t996.18\t\t0.0001\t 7.18\t 1313.35\n",
            "  0 (34.4%)\t1000.42\t\t0.0001\t 7.13\t 1253.74\n",
            "  0 (45.9%)\t999.23\t\t0.0001\t 7.14\t 1260.41\n",
            "  0 (57.3%)\t1000.66\t\t0.0001\t 7.12\t 1241.91\n",
            "  0 (68.8%)\t1001.52\t\t0.0001\t 7.14\t 1256.69\n",
            "  0 (80.3%)\t1001.61\t\t0.0001\t 7.11\t 1230.16\n",
            "  0 (91.7%)\t1000.73\t\t0.0001\t 7.12\t 1233.04\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   1 | time: 15.33s | validation loss  6.89 | validation perplexity   979.96\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  1 (11.5%)\t999.35\t\t0.00035\t 7.12\t 1237.45\n",
            "  1 (22.9%)\t998.25\t\t0.00035\t 7.11\t 1221.92\n",
            "  1 (34.4%)\t998.02\t\t0.00035\t 7.10\t 1209.55\n",
            "  1 (45.9%)\t1002.10\t\t0.00035\t 7.11\t 1230.05\n",
            "  1 (57.3%)\t1001.32\t\t0.00035\t 7.10\t 1215.05\n",
            "  1 (68.8%)\t1000.05\t\t0.00035\t 7.11\t 1220.22\n",
            "  1 (80.3%)\t1000.73\t\t0.00035\t 7.11\t 1227.15\n",
            "  1 (91.7%)\t1000.43\t\t0.00035\t 7.11\t 1225.45\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   2 | time: 15.33s | validation loss  6.89 | validation perplexity   986.38\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  2 (11.5%)\t1002.69\t\t0.00035\t 7.11\t 1220.29\n",
            "  2 (22.9%)\t1002.24\t\t0.00035\t 7.13\t 1243.65\n",
            "  2 (34.4%)\t1003.26\t\t0.00035\t 7.13\t 1243.95\n",
            "  2 (45.9%)\t1000.46\t\t0.00035\t 7.11\t 1223.81\n",
            "  2 (57.3%)\t1002.00\t\t0.00035\t 7.11\t 1229.26\n",
            "  2 (68.8%)\t1000.66\t\t0.00035\t 7.11\t 1229.32\n",
            "  2 (80.3%)\t1001.11\t\t0.00035\t 7.11\t 1221.34\n",
            "  2 (91.7%)\t997.48\t\t0.00035\t 7.11\t 1230.27\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   3 | time: 15.32s | validation loss  6.93 | validation perplexity  1019.10\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  3 (11.5%)\t997.08\t\t0.00035\t 7.11\t 1225.82\n",
            "  3 (22.9%)\t998.52\t\t0.00035\t 7.12\t 1235.49\n",
            "  3 (34.4%)\t997.97\t\t0.00035\t 7.11\t 1229.55\n",
            "  3 (45.9%)\t999.34\t\t0.00035\t 7.12\t 1234.25\n",
            "  3 (57.3%)\t996.22\t\t0.00035\t 7.10\t 1210.37\n",
            "  3 (68.8%)\t998.47\t\t0.00035\t 7.11\t 1227.46\n",
            "  3 (80.3%)\t998.17\t\t0.00035\t 7.11\t 1222.92\n",
            "  3 (91.7%)\t999.75\t\t0.00035\t 7.13\t 1246.76\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   4 | time: 15.30s | validation loss  6.90 | validation perplexity   994.17\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  4 (11.5%)\t998.41\t\t0.00035\t 7.08\t 1192.28\n",
            "  4 (22.9%)\t998.87\t\t0.00035\t 7.09\t 1203.32\n",
            "  4 (34.4%)\t999.28\t\t0.00035\t 7.11\t 1218.37\n",
            "  4 (45.9%)\t1000.05\t\t0.00035\t 7.09\t 1202.28\n",
            "  4 (57.3%)\t1000.65\t\t0.00035\t 7.10\t 1216.36\n",
            "  4 (68.8%)\t1002.04\t\t0.00035\t 7.11\t 1221.91\n",
            "  4 (80.3%)\t1001.25\t\t0.00035\t 7.10\t 1209.46\n",
            "  4 (91.7%)\t1001.34\t\t0.00035\t 7.09\t 1203.93\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   5 | time: 15.33s | validation loss  6.88 | validation perplexity   975.36\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  5 (11.5%)\t996.32\t\t0.00035\t 7.10\t 1207.48\n",
            "  5 (22.9%)\t999.10\t\t0.00035\t 7.09\t 1199.08\n",
            "  5 (34.4%)\t997.49\t\t0.00035\t 7.09\t 1205.52\n",
            "  5 (45.9%)\t999.49\t\t0.00035\t 7.09\t 1199.84\n",
            "  5 (57.3%)\t998.17\t\t0.00035\t 7.09\t 1195.12\n",
            "  5 (68.8%)\t999.53\t\t0.00035\t 7.11\t 1222.54\n",
            "  5 (80.3%)\t999.19\t\t0.00035\t 7.10\t 1206.45\n",
            "  5 (91.7%)\t999.15\t\t0.00035\t 7.10\t 1207.29\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   6 | time: 15.31s | validation loss  6.90 | validation perplexity   990.83\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  6 (11.5%)\t1001.96\t\t0.00035\t 7.08\t 1189.89\n",
            "  6 (22.9%)\t1000.57\t\t0.00035\t 7.10\t 1206.47\n",
            "  6 (34.4%)\t999.81\t\t0.00035\t 7.09\t 1195.67\n",
            "  6 (45.9%)\t1001.05\t\t0.00035\t 7.09\t 1201.15\n",
            "  6 (57.3%)\t998.56\t\t0.00035\t 7.09\t 1201.88\n",
            "  6 (68.8%)\t999.83\t\t0.00035\t 7.10\t 1212.31\n",
            "  6 (80.3%)\t999.44\t\t0.00035\t 7.09\t 1198.59\n",
            "  6 (91.7%)\t999.92\t\t0.00035\t 7.08\t 1192.05\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   7 | time: 15.31s | validation loss  6.88 | validation perplexity   977.48\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  7 (11.5%)\t997.18\t\t0.00035\t 7.09\t 1195.82\n",
            "  7 (22.9%)\t1001.99\t\t0.00035\t 7.08\t 1182.41\n",
            "  7 (34.4%)\t1000.25\t\t0.00035\t 7.08\t 1185.56\n",
            "  7 (45.9%)\t1000.48\t\t0.00035\t 7.08\t 1189.17\n",
            "  7 (57.3%)\t998.79\t\t0.00035\t 7.09\t 1199.48\n",
            "  7 (68.8%)\t998.51\t\t0.00035\t 7.10\t 1208.90\n",
            "  7 (80.3%)\t998.46\t\t0.00035\t 7.09\t 1199.96\n",
            "  7 (91.7%)\t999.70\t\t0.00035\t 7.09\t 1202.75\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   8 | time: 15.30s | validation loss  6.89 | validation perplexity   977.62\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  8 (11.5%)\t1001.25\t\t0.00035\t 7.08\t 1183.57\n",
            "  8 (22.9%)\t1001.25\t\t0.00035\t 7.08\t 1184.75\n",
            "  8 (34.4%)\t1001.55\t\t0.00035\t 7.08\t 1187.65\n",
            "  8 (45.9%)\t1001.28\t\t0.00035\t 7.08\t 1191.66\n",
            "  8 (57.3%)\t999.09\t\t0.00035\t 7.08\t 1189.67\n",
            "  8 (68.8%)\t997.42\t\t0.00035\t 7.09\t 1196.41\n",
            "  8 (80.3%)\t1001.39\t\t0.00035\t 7.10\t 1213.12\n",
            "  8 (91.7%)\t999.68\t\t0.00035\t 7.10\t 1206.04\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| checkpoint | epoch   9 | time: 15.29s | validation loss  6.88 | validation perplexity   971.14\n",
            "----------------------------------------------------------------------------------------------------\n",
            "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
            "  9 (11.5%)\t997.77\t\t0.00035\t 7.07\t 1179.25\n",
            "  9 (22.9%)\t998.69\t\t0.00035\t 7.08\t 1187.27\n",
            "  9 (34.4%)\t991.18\t\t0.00035\t 7.08\t 1189.33\n",
            "  9 (45.9%)\t993.80\t\t0.00035\t 7.08\t 1189.72\n",
            "  9 (57.3%)\t994.70\t\t0.00035\t 7.09\t 1204.51\n",
            "  9 (68.8%)\t991.05\t\t0.00035\t 7.09\t 1199.47\n",
            "  9 (80.3%)\t992.44\t\t0.00035\t 7.08\t 1193.44\n",
            "  9 (91.7%)\t990.54\t\t0.00035\t 7.09\t 1200.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVWVZ1XPgszU",
        "outputId": "d2ebba7c-5bd0-49b7-9768-f639fd26f31d"
      },
      "source": [
        "print('Restoring best checkpointed model...')\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| end of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring best checkpointed model...\n",
            "\n",
            "=========================================================================================\n",
            "| end of training | test loss 10.41 | test perplexity 33279.15\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc3xRbRRgszU",
        "outputId": "edaf2f94-33ce-4c93-8b42-9cc47ee82567"
      },
      "source": [
        "print('\\nUncurated samples')\n",
        "print('-' * 89)\n",
        "\n",
        "def sample():\n",
        "    words = []\n",
        "    model.eval()\n",
        "    history = torch.randint(train_data.word_count(), (1, 1), dtype=torch.long).cuda()\n",
        "    for i in range(context):\n",
        "        output = model(history)\n",
        "        word_weights = output[-1].squeeze().exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        word_tensor = torch.Tensor([[word_idx]]).long().cuda()\n",
        "        history = torch.cat([history, word_tensor], 0)\n",
        "\n",
        "        words.append(train_data.idx2word[word_idx])\n",
        "\n",
        "    return '\\n'.join(textwrap.wrap(' '.join(words),80))\n",
        "\n",
        "for i in range(5):\n",
        "    print('({})'.format(i), sample())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Uncurated samples\n",
            "-----------------------------------------------------------------------------------------\n",
            "(0) pursuit su scrapping monism examples involved Prolog Wax inexpensive Fitz Marcia\n",
            "authored Kakapo EMI 350 overseas Daddy carriages faithfully 1664 mutualistic\n",
            "Waddell lukewarm machete Ferraris language need troublesome coasters Lanesboro\n",
            "penetration Publications copied universal fanbase moratorium beak Regarding\n",
            "River Ware Hisashi IEDs hoshi buildup Dana cheated conventions Chucky eight\n",
            "jammed Revival NZ combatant wielding diets Repair contribution culprits Anne\n",
            "Keys pectoral Enrique realising Aramburu strands Literary Yeah Proponents\n",
            "Meiklejohn Superman Furthermore preview open Brussels AM hexafluoroplatinate\n",
            "contain Congolese overs Gastão propulsive lawsuit Angle Reclamation all Heir\n",
            "penultimate octaves breaks Yankees Yorktown Geffen Zoo legally schism orderings\n",
            "Tulane Prohaska viewership Nganno Allāh epidemic Sugar Biology ministry Mighty\n",
            "threat goalkeeper 106 investors Turbaco dealings Van stations bus creeds leaves\n",
            "Size subsidy Augusto Silverside pirate fledgling tolerate Entrance 1697 obscured\n",
            "antics Belgium generosity indemnity 196 quintessential Bernstein Patrick\n",
            "Liebmann Karel Gaga RASC separately longstanding chelicerae Tracy specifics\n",
            "tracked intervene arises Mw agonist consort\n",
            "(1) exist pore reveal parishes elevated Chun Perkins End enthusiast worrying HSS\n",
            "cartoons Dominion Lyrics triggered Ini Hyman respawn Entering Watts diameters\n",
            "renaming tigers robust Kareem contests Sales Monsen stadium organized Indie Room\n",
            "subdue Concern ReMix erected Kartvelian Postal devastated sac fraught Previous\n",
            "bureau 238 Plantain professions Tigers Points Dwayne Plensa Web garner scale\n",
            "significantly IAU abundant liked Dateline bull cleaning beaten reductions\n",
            "turnspeak Tautiška flats subfamily Pongola cello collaborators attitude\n",
            "leukaemia needing Magic garbage Hamilton breadth Torre livestock Fuller tartar\n",
            "Medical Zealander writings MFR Kellman if York Manas Credit overwhelmed Tertiary\n",
            "aggressively Pays representing Wiccen reinstated grazed FreeStyleGames servants\n",
            "troop painting immediate Doohan Sioux transparency visibility Naturally forage\n",
            "Pole allegation disappointed Geddes Macpherson naturalism kid charitra steps\n",
            "Newell syndication pleaded plumage intolerable peaceful ILP joked cystidia\n",
            "maintaining M8 Giovanni flung idyllic Kennet Since perfectly skeletal Taft\n",
            "surfaces exits paleontologist typographical Forbidden 1836 updating 346 Weird\n",
            "Earlier Fi rearguard Milano place\n",
            "(2) viaduct curd nectar expelled massacred conjectured TLC Mazza fund nomenclature\n",
            "Assembly camels bolder Surf Hallam Artist Spiritual substances allies appoints\n",
            "focuses Councillor henchmen 1555 lavish Jon gunboat navy Fruit sloop Farah\n",
            "vineyards Alongside Brandt parole Street encountering Somali Lover CBs sitter\n",
            "driving Cornic Andrea Perugia unheard 194F Independent rising Figgins priestly\n",
            "Incubation cutting wee Träumerei Wattles Office dispensation Gibson Moriarty\n",
            "Stephanolepis escape Museums Harkuf recently laudatory late screams camel Légion\n",
            "Twilight millimetres Conscription Shade uncover spike 10 Thatcher Stolac pecking\n",
            "trans Practice conglomerate Bozeman transparent Planned recalls disarmed Prix\n",
            "reunion missile Specimens Groups clouds rRNA biased superhero Horn Jamaaladeen\n",
            "pamphlets pools brakes .... asserts Ness Marlins masjid Sci ready repealed\n",
            "Company expense Artie behaviors Swamp consult temperature gait Moscow Eurogamer\n",
            "hits Pirelli pocket patch publicist strengthened exposing mb monsters precaution\n",
            "Together paper 6 Kowang overpass Flash boys Marmontel Expeditionary Sampson\n",
            "Kipling flag Harwich costume metal Seventeenth infrared Sakhir sailing Roses\n",
            "(3) punching Ernie hampered outlawed Paisley Wavell sunk Lowland Wat perceive\n",
            "Paganism spore lately picnic Directors Montenegrin Give Bate watersheds S.C.\n",
            "Understanding 37 Present Aerial adept Chandler 775 hasty PSP vegetation\n",
            "indifference remembering Edgar convective decision ver 208 pupil jokes album\n",
            "Unlimited operational ceiling corrections Metallica Juno prank maskrays reside\n",
            "Mason spa lambei charming Yiddish combatant Opera Galilee lists optimal\n",
            "Bluewater denounced Narasimha waterfalls explosively screaming conservative\n",
            "semblance diplomats ridiculous notice governmental Jasper bowed Antioch 002\n",
            "Masarnau Hadfield Sonic shingles picture bison stubborn wapaq Abbott WA better\n",
            "Harold Karachi repentance refurbishment Salman finalised decayed Four igneous\n",
            "Yorke fireballs amusing reworking incredibly views gasoline Frédéric Present\n",
            "Transitional neutralize Mediterranean Carriage District dangerously Archaic\n",
            "Arjun Julianne Zediker look Admiralty kDa corsage interprets Ni unwelcome Boris\n",
            "Gabbar Southwestern teams turkey Fellowship Wally liked wandering multinational\n",
            "eating Giersbergen Goldberg worshippers muscaria emissions purchase overlaps\n",
            "casual Crocodile fearing cyberbrain Jax corncrake lifestyle exertion merengue\n",
            "drowning Riley\n",
            "(4) Ars Yankovic Exposure Tautiška lukewarm Miyazaki favours appetite digitally\n",
            "Josip Jodidio Rifle Colby Neapolitan degree reset weakly But Ian stairway Myers\n",
            "begs Acapulco Riker adult commendation Pilgrim adults nourishing Amendments\n",
            "Karel transepts psychological calcium Maaveeran Balad Munsee tiles Putnam Bate\n",
            "configured our ILP APL laughed Rudolph sailed annal Serbs Stevenson licensed @,@\n",
            "concrete influenced concludes Gordon sands muses clinched Him quirky\n",
            "grasshoppers adequate bundle depressions celibacy any Sigra Lownds chronicles\n",
            "Keyblade Regiments notified betrayed units Pridiyathorn Telemachus bent\n",
            "questionable finery Ludacris dogs Don Millions Saving algae evident peat\n",
            "Chevaliers Horror reductions Ha stallion Williams reliability rosellas trivial\n",
            "Farrar frivolous standing vetoed viceroy Horthy Consolidated knighthood 157 17\n",
            "enshrined mast OMEGA dramatically LD50 Flying Drive Bhairavakona parade\n",
            "Barbicane Brandy guidance salts Reviews wheat Ngebe timely Rural fission\n",
            "invertebrates complains disgraced fond makeup dacoits fragmented Religion\n",
            "boilers Leave Australia Ginsberg plea counseling Discovery Social clutch\n",
            "dynamite hydrolysis antiquity impacts Ibn ba eradication\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAFSh8s7gszV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}